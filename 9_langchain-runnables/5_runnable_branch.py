from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
from langchain.schema.runnable import (
    RunnableSequence,
    RunnableParallel,
    RunnablePassthrough,
    RunnableBranch,
    RunnableLambda,
)

load_dotenv()

# slide 74

# RunnableBranch:
# RunnableBranch is a control flow component in LangChain that allows you to conditionally
# route input data to different chains or runnables based on custom logic.

# It functions like an if/elif/else block for chains â€”where you define a set of condition functions,
# each associated with a runnable (e.g., LLM call, prompt chain, or tool). The first matching
# condition is executed. If no condition matches, a default runnable is used (if provided).


prompt1 = PromptTemplate(
    template="Write a detailed report on {topic}", input_variables=["topic"]
)

prompt2 = PromptTemplate(
    template="Summarize the following text \n {text}", input_variables=["text"]
)

model = ChatOpenAI()

parser = StrOutputParser()

report_gen_chain = prompt1 | model | parser

branch_chain = RunnableBranch(
    (lambda x: len(x.split()) > 300, prompt2 | model | parser),
    RunnablePassthrough(),
)

final_chain = RunnableSequence(report_gen_chain, branch_chain)

print(final_chain.invoke({"topic": "Russia vs Ukraine"}))

